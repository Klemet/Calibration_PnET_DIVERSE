{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e2d598b-5bdc-4def-b411-5ebe9253e781",
   "metadata": {},
   "source": [
    "# Getting the climate and soil data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d847bc-6908-447d-9c42-0ae8504986e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now that we have initial parameters for our species, we need to define the landscape in which the calibration will take place.\n",
    "\n",
    "To adapt to the Python functions I've made for these Jupyter Notebooks (see [here](./functionsForCalibration.py)), simulations will take place on a single cell. \n",
    "\n",
    "What we need is :\n",
    "\n",
    "- The climate file (containing all of the climate variables need by PnET)\n",
    "- The ecoregion file and PnET ecoregion parameters (to decide which ones will be used during the calibration)\n",
    "- Initial communities files (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5e7c5-aa53-44c6-b315-1889098b28af",
   "metadata": {},
   "source": [
    "## The climate file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d31f9-f11d-4373-98e7-21eb57bc7af6",
   "metadata": {},
   "source": [
    "See [Gustafson and Miranda (2023)](./ReferencesAndData/Documentation/Gustafson2024PnETUserGuide.pdf) for detailed information about what the climate file used in PnET simulations must contain.\n",
    "\n",
    "[Gustafson and Miranda (2023)](./ReferencesAndData/Documentation/Gustafson2024PnETUserGuide.pdf) recommends using a constant climate for calibrating (p. 69). Gustafson also recommends using an \"ideal\" climate at some points, but deciding what is an ideal climate for a given species is a difficult proposition. As we are going to compare the growth curves generated by PnET Succession with the growth curves estimated from data of the National Forest Inventory (NFI) of Canada (see [](./5.Other_important_data_before_starting.ipynb)), the idea will be rather to get long-term monthly averages (as recommanded by [Gustafson and Miranda (2023)](./ReferencesAndData/Documentation/Gustafson2024PnETUserGuide.pdf) p. 73) in a region that is pretty representative climate of the study area of interest.\n",
    "\n",
    "PnET Succession requires 5 climate variables to function :\n",
    "\n",
    "- Maximum Monthly temperature (°C)\n",
    "- Minimum Monthly temperature (°C)\n",
    "- Photosynthetically Active Radiation (umol/m2/s)\n",
    "- Sum of precipitations during the month (mm)\n",
    "- Mean monthly atmospheric CO2 concentration (ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1138b-81c8-401c-97a2-d2fa4feb52e2",
   "metadata": {},
   "source": [
    "### Climate data location for the calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617de8c2-5cd2-4bea-9b62-e2c3a6ed61fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Since we're doing calibration simulation here, we don't have a particular place where our simulation take place. It's up to us to choose where the climate data comes from, and there is no a \"best\" way to do things.\n",
    "\n",
    "It's surely most likely better to use climate conditions that are closer to the average of the conditions we want to simulate in other LANDIS-II simulation rather than to extremes (especially for this first step of calibration). We also want to make sure that the location we use ca be inputted in FVSon.\n",
    "\n",
    "I propose that we use an area near the border between the boral and temperate forest, and near the center of Ontario. It will be arbitrary.\n",
    "\n",
    "Here is a map from of the forests of Ontario from the 2016 forest report of Ontario :\n",
    "\n",
    "![](https://files.ontario.ca/1a-forestregion-map_e_1.png)\n",
    "\n",
    "The city of [Chapleau](https://www.openstreetmap.org/#map=12/47.8416/-83.4106) seems to be located near the limit between the boreal and temperate forest. I created a simple shapefile polygon around the city that will serve to clip the climate data we want. See the next cell for a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a16092e-12af-4161-94b1-85fb575942c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide_input",
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_b93358a66b96a86acd1e316a37adee0f {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_b93358a66b96a86acd1e316a37adee0f&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_b93358a66b96a86acd1e316a37adee0f = L.map(\n",
       "                &quot;map_b93358a66b96a86acd1e316a37adee0f&quot;,\n",
       "                {\n",
       "                    center: [47.824934688075345, -83.36832547778936],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    ...{\n",
       "  &quot;zoom&quot;: 10,\n",
       "  &quot;zoomControl&quot;: true,\n",
       "  &quot;preferCanvas&quot;: false,\n",
       "}\n",
       "\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_2792db0de01d479094d79b1d42220a3f = L.tileLayer(\n",
       "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {\n",
       "  &quot;minZoom&quot;: 0,\n",
       "  &quot;maxZoom&quot;: 19,\n",
       "  &quot;maxNativeZoom&quot;: 19,\n",
       "  &quot;noWrap&quot;: false,\n",
       "  &quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;,\n",
       "  &quot;subdomains&quot;: &quot;abc&quot;,\n",
       "  &quot;detectRetina&quot;: false,\n",
       "  &quot;tms&quot;: false,\n",
       "  &quot;opacity&quot;: 1,\n",
       "}\n",
       "\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_2792db0de01d479094d79b1d42220a3f.addTo(map_b93358a66b96a86acd1e316a37adee0f);\n",
       "        \n",
       "    \n",
       "        function geo_json_f7d9ce08bc740695be45fdb31bdf01e5_styler(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;color&quot;: &quot;#2e3440&quot;, &quot;fillColor&quot;: &quot;#ebcb8b&quot;, &quot;fillOpacity&quot;: 0.4, &quot;weight&quot;: 1};\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function geo_json_f7d9ce08bc740695be45fdb31bdf01e5_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "            });\n",
       "        };\n",
       "        var geo_json_f7d9ce08bc740695be45fdb31bdf01e5 = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_f7d9ce08bc740695be45fdb31bdf01e5_onEachFeature,\n",
       "            \n",
       "                style: geo_json_f7d9ce08bc740695be45fdb31bdf01e5_styler,\n",
       "            ...{\n",
       "}\n",
       "        });\n",
       "\n",
       "        function geo_json_f7d9ce08bc740695be45fdb31bdf01e5_add (data) {\n",
       "            geo_json_f7d9ce08bc740695be45fdb31bdf01e5\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_f7d9ce08bc740695be45fdb31bdf01e5_add({&quot;bbox&quot;: [-83.51730371900837, 47.738566201858305, -83.21934723657034, 47.91130317429239], &quot;features&quot;: [{&quot;bbox&quot;: [-83.51730371900837, 47.738566201858305, -83.21934723657034, 47.91130317429239], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[-83.51730371900837, 47.738566201858305], [-83.51730371900837, 47.91130317429239], [-83.21934723657034, 47.91130317429239], [-83.21934723657034, 47.738566201858305], [-83.51730371900837, 47.738566201858305]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;0&quot;, &quot;properties&quot;: {&quot;AREA&quot;: 0.051468100693455, &quot;CNTX&quot;: -83.36832547778936, &quot;CNTY&quot;: 47.824934688075345, &quot;HEIGHT&quot;: 0.172736972434087, &quot;MAXX&quot;: -83.21934723657034, &quot;MAXY&quot;: 47.91130317429239, &quot;MINX&quot;: -83.51730371900837, &quot;MINY&quot;: 47.738566201858305, &quot;PERIM&quot;: 0.941386909744224, &quot;WIDTH&quot;: 0.297956482438025}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "\n",
       "        \n",
       "    \n",
       "            geo_json_f7d9ce08bc740695be45fdb31bdf01e5.addTo(map_b93358a66b96a86acd1e316a37adee0f);\n",
       "        \n",
       "    \n",
       "            var layer_control_cb8fa9d8884d268f34f9b1196002686f_layers = {\n",
       "                base_layers : {\n",
       "                    &quot;openstreetmap&quot; : tile_layer_2792db0de01d479094d79b1d42220a3f,\n",
       "                },\n",
       "                overlays :  {\n",
       "                    &quot;Polygons&quot; : geo_json_f7d9ce08bc740695be45fdb31bdf01e5,\n",
       "                },\n",
       "            };\n",
       "            let layer_control_cb8fa9d8884d268f34f9b1196002686f = L.control.layers(\n",
       "                layer_control_cb8fa9d8884d268f34f9b1196002686f_layers.base_layers,\n",
       "                layer_control_cb8fa9d8884d268f34f9b1196002686f_layers.overlays,\n",
       "                {\n",
       "  &quot;position&quot;: &quot;topright&quot;,\n",
       "  &quot;collapsed&quot;: true,\n",
       "  &quot;autoZIndex&quot;: true,\n",
       "}\n",
       "            ).addTo(map_b93358a66b96a86acd1e316a37adee0f);\n",
       "\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f153b782500>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displays a map where the shapefile that defines the boundaries of the climate data we wanna use is\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.features import GeoJsonPopup, GeoJsonTooltip\n",
    "\n",
    "# Load the shapefile\n",
    "# Replace 'path_to_shapefile.shp' with your actual shapefile path\n",
    "gdf = gpd.read_file(\"./ReferencesAndData/SpatialBoundaries/ChapleauBoundariesClimate.shp\")\n",
    "\n",
    "# Check the CRS (Coordinate Reference System) of the shapefile\n",
    "# If not in WGS84 (EPSG:4326), reproject it\n",
    "if gdf.crs != 'EPSG:4326':\n",
    "    gdf = gdf.to_crs('EPSG:4326')\n",
    "\n",
    "# Create a map centered on the mean of the shapefile bounds\n",
    "center_lat = gdf.unary_union.centroid.y\n",
    "center_lon = gdf.unary_union.centroid.x\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=10)\n",
    "\n",
    "# Add the GeoJSON data to the map with some styling\n",
    "folium.GeoJson(\n",
    "    gdf,\n",
    "    name='Polygons',\n",
    "    style_function=lambda x: {\n",
    "        'fillColor': '#ebcb8b',\n",
    "        'color': '#2e3440',\n",
    "        'weight': 1,\n",
    "        'fillOpacity': 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Alternatively, you can use this simpler approach which works in newer versions of JupyterLab\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65dbda1-54b5-434c-8602-78617a5f1527",
   "metadata": {},
   "source": [
    "### Climate data source for 🌡️ temperature, 🌧️ precipitation and ☀ solar radiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227779a2-bc97-44d0-9c71-2e466efa26b5",
   "metadata": {},
   "source": [
    "#### Choosing the right source - going with ESGF data\n",
    "\n",
    "There are many potential sources of climate data. I've had to juggle with many of them before finding something that fitted our objective here, but also looking more at the long term of the DIVERSE project.\n",
    "\n",
    "I first wanted to use data from Climate Canada, as they produce outputs of several Global Climate models from CMIP6 at a very small resolution (downscaled), and accessible through the OpenDAP protocol (which allow us to only download the chunks of the data we need). However, their model lacked some of the variables needed for PnET-Succession, especially the solar radiation. This required to use statistical models to predict this variable based on other datasets, which was really not ideal. In addition, their data was monthly, and we were going to require daily data to accomodate some LANDIS-II extensions we were going to use with PnET-Succession further down the road (e.g. BFOLDS Fire).\n",
    "\n",
    "I then went on to use data from the ESGF (Earth System Grid Federation); first manually from their \"[Metagrid](https://esgf.github.io/nodes.html)\" online interface, and then via scripts using their [intake-esgf](https://github.com/esgf2-us/intake-esgf) python package. This allowed us to access outputs from a very large array of GCMs, different variables, etc, at a daily scale, and with the OpenDAP protocol. However, the gridded data here is 100 x 100km of resolution, which is pretty coarse.\n",
    "\n",
    "I will keep on going with this data from the ESGF in this notebook, so that it is easily re-usable for research teams in other parts of the world (since data from the ESGF covers all of the world). However, for the DIVERSE project, we will most likely use the BioSIM canadian model to downscale climate data, and specially get different climate data for areas with a slope/aspect that can affect local temperature and solar radiation that is received by trees.\n",
    "\n",
    "#### Climate model that we are using here\n",
    "\n",
    "There are many Global Climate Models (GCMs), and it's sometimes difficult to choose the right one. Here, I've choosen to go with TaiESM1, which was the closest to the median accross models from CMIP6 for the temperature and precipitations for our first tests with the CanDCS-M6 dataset. According to the litterature, TaiESM1 also seems to have a pretty good performance overall when compared in GCM ensembles (see [here](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020MS002353)). TaiESM1 also have a lot of data for different SSPs scenarios on the ESGF data repositories, which will be useful for the later steps of the calibration, but also for the DIVERSE project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a4c5d-5a92-4992-986f-35a8843a2639",
   "metadata": {},
   "source": [
    ":::{tip} Why we are using a single climate model (instead of several together) here\n",
    "Climate scientists often recommand to use the results from several climate models (GCMs) at once so as to explore the influence of the variability in results that exists between models, and the incertainty that results from it.\n",
    "\n",
    "Here, I choose to use the results from a single climate model for several reasons :\n",
    "\n",
    "- The goal of this calibration is not to explore climate change predictions, but simply to calibrate the growth and dynamic of our tree species. For that, we will use historical data simulated by the model we chose (TaiESM1).\n",
    "- This calibration is already quite complex, and I'd like to keep it as simple as possible.\n",
    "- Using several models at once requires one of two things : either using a single climate file whose values are averaged along the values simulated by the different GCMs, which results in very, very \"mild\" climate conditions (since all extreme variations are not synchronized between models); or to use several climate files and create several PnET-Succession growth curves at each point (which makes things quite complicated).\n",
    "\n",
    "However, once you have calibrated PnET Succession and will use it in your LANDIS-II simulations, it might be good to generate different climate files based on different climate models in order to create \"climate replicates\" for your LANDIS-II simulation that will allow you to generate an envellope of variability/uncertainty on your LANDIS-II results. To that end, you can use the scripts of this page to generate several climate files for different climate models available in ESGF (be warned that not all models have all of the variables you will need for all of the climate scenario you will want to explore). You can also use the functions from the Xclim python package that will allow you to select a smaller number of models that will represent your climate data properly (see [here](https://xclim.readthedocs.io/en/stable/apidoc/xclim.ensembles.html#xclim.ensembles._reduce.kmeans_reduce_ensemble); you will need to create a large ensemble of climate data to use these functions).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb95231-4d89-458e-b484-77f6ea4f1c29",
   "metadata": {},
   "source": [
    "#### Extraction of 🌡️ temperature, 🌧️ precipitation and ☀ solar radiation data for Ontario, 1950-2100 from [ESGF](https://esgf.github.io/nodes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c8208-49ae-45e2-b95a-c54057df5df7",
   "metadata": {},
   "source": [
    "The goal here is to extract the data for :\n",
    "\n",
    "- The TaiESM1 model\n",
    "- 4 variables (Tmax, Tmin, precip, PAR)\n",
    "- Historical range (1950-2010)\n",
    "- At a daily frequency\n",
    "- At a 100x100km resolution, but only inside our polygon (see above), and only for a single cell that is closest to the polygon centroid in the end (since we will only simulate one cell at a time during the calibration, we don't need more than one climate stream for one ecoregion)\n",
    "\n",
    "We extract the raw variables first for each, who each have particular names for CMIP6 data like this one :\n",
    "- tasmax = maximum daily temperature in kelvin\n",
    "- tasmin = minimum daily temperature in kelvin\n",
    "- prc = daily precipitations in kg m-2 s-1\n",
    "- rsds = average daily downwelling shortwave solar radiation in W/m2\n",
    "\n",
    "To be certain of these units, we can check the metadata/attributes of the objects manipulated during this script, or on the ESGF metagrid.\n",
    "\n",
    "Other variables are available at a daily frequency that can be used for other LANDIS-II extensions if you need them. For example, the variables necessary for BFOLDS fire :\n",
    "\n",
    "- hur, the relative humidty in percentage\n",
    "- ua, the eastward wind speed in m/s\n",
    "- va, the northward wind speed in m/s\n",
    "\n",
    "For other variables codes, see sites such as [this one](https://pcmdi.llnl.gov/mips/cmip3/variableList.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47a2f28-ed04-42ca-886a-e779d8d3fa43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02afc94fcaed46e6907e874e755be25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Searching indices:   0%|          |0/2 [       ?index/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63de39baf8674e55a02bae7b861c7517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Get file information:   0%|          |0/2 [       ?index/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here, we get the URLs to download the data we need through the OpenDAP protocol,\n",
    "# by looking at the datasets available in the ESGF nodes\n",
    "# WARNING : if you have a VPN on, this might result in error as nodes might refuse the connection.\n",
    "# Simply disable the VPN and try again.\n",
    "\n",
    "from functionsForCalibration import *\n",
    "from intake_esgf import ESGFCatalog\n",
    "import intake_esgf\n",
    "\n",
    "# Initialize catalog\n",
    "cat = ESGFCatalog()\n",
    "\n",
    "# Search for the variables we need for TaiESM1, historical data, and daily frequency\n",
    "# See cell above for infos about the variables\n",
    "cat.search(\n",
    "    experiment_id=\"historical\",\n",
    "    source_id=\"TaiESM1\",\n",
    "    frequency=\"day\",\n",
    "    # variable_id=[\"rsds\", \"prc\", \"tasmin\", \"tasmax\", \"hur\", \"ua\", \"va\"], # Variables for BFOLDS added\n",
    "    variable_id=[\"rsds\", \"prc\", \"tasmin\", \"tasmax\"]\n",
    ")\n",
    "\n",
    "# Making a dictionnary with the path to then access the data in xarray with OpenDAP,\n",
    "# we can see that the function only return one single .nc file per dataset, for a single\n",
    "# time period. The time period for each variable seems to be pretty random.\n",
    "paths = cat.to_path_dict(prefer_streaming=True)\n",
    "# print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ced712-88b2-47fa-8ede-6f24ac79dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we're going to loop around the OpenDAP paths and extract the data for the years we want and in\n",
    "# the polygon we defined earlier, thanks to a custom function.\n",
    "# WARNING : you might get errors if there are paths in the \"paths\" object that we created just before that\n",
    "# are not OpenDAP URL paths. It might happen since the to_path_dict function of intake_esgf is still experimental.\n",
    "# If this happens, you need to clean the dictionnary to only have the OpenDAP URLs you need.\n",
    "from functionsForCalibration import *\n",
    "\n",
    "# We initialize the dictionnary that are going to fill\n",
    "# A key = one variable. The value will be a dataframe filled with the data from the variable.\n",
    "dictOfDataFrames = dict()\n",
    "\n",
    "for variable in paths.keys():\n",
    "    # Since the data is often fragmented accross several .nc files that correspond to different ranges of year\n",
    "    # for the data, we need to first initialize a panda dataframe, and then add rows to it.\n",
    "    # That's what we do here.\n",
    "    variableInitialized = False\n",
    "    for path in paths[variable]:\n",
    "        if not variableInitialized:\n",
    "            dictOfDataFrames[variable] = GetDataFromESGFdataset(path,\n",
    "                                            yearStart = 1950,\n",
    "                                            yearStop = 2010,\n",
    "                                            pathOfShapefileForSubsetting = \"./ReferencesAndData/SpatialBoundaries/ChapleauBoundariesClimate.shp\",\n",
    "                                            nameOfVariable = variable,\n",
    "                                            verbose = False,\n",
    "                                            enableWarnings = False)\n",
    "            if not dictOfDataFrames[variable].empty:\n",
    "                variableInitialized = True\n",
    "        else:\n",
    "            additionalData = GetDataFromESGFdataset(path,\n",
    "                                            yearStart = 1950,\n",
    "                                            yearStop = 2010,\n",
    "                                            pathOfShapefileForSubsetting = \"./ReferencesAndData/SpatialBoundaries/ChapleauBoundariesClimate.shp\",\n",
    "                                            nameOfVariable = variable,\n",
    "                                            verbose = False,\n",
    "                                            enableWarnings = False)\n",
    "            # The complex thing in pd.concat is used to deal with cases when GetDataFromESGFdataset returned an empty dataframe,\n",
    "            # which happens when the .nc file didn't corresponded to the years we wanted.\n",
    "            dictOfDataFrames[variable] = pd.concat([dictOfDataFrames[variable], additionalData if not additionalData.empty else None], ignore_index=True)\n",
    "        # We sort the values by day, because they might be out of order.\n",
    "        dictOfDataFrames[variable] = dictOfDataFrames[variable].sort_values(['year', 'month', 'day'])\n",
    "        # print(dictOfDataFrames[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bba2959-3224-46f4-9033-8407089a99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the raw data accross several 100x100km grid cells for all variables necessary for PnET,\n",
    "# we simply have to transform the measures if needed (e.g. PAR into umol/m2.s for the right radiation range instead of rsds which is W/m2)\n",
    "# and then put everything in the right format for the LANDIS-II v8 climate library, and finally output things as .csv\n",
    "# for latter use\n",
    "import copy\n",
    "\n",
    "##################################\n",
    "# Restricting data to only one grid cell\n",
    "# (since we will only simulate on cell in our monocultures)\n",
    "##################################\n",
    "\n",
    "# We get the unique coordinates in the dataframe\n",
    "uniqueCoordinatesInDataFrames = extract_unique_coordinates(dictOfDataFrames[\"rsds\"])\n",
    "# Then, we choose the cell that is closest to the centroid of our chapleau region we've been using\n",
    "bestCoordinatePair = find_closest_coordinate_to_polygon_center(\"./ReferencesAndData/SpatialBoundaries/ChapleauBoundariesClimate.shp\",\n",
    "                                                                uniqueCoordinatesInDataFrames)\n",
    "# Then, we remove all references to other coordinate pairs\n",
    "# Extract the latitude and longitude from the target coordinate\n",
    "target_lat, target_lon = bestCoordinatePair\n",
    "dictOfDataFramesSubset = copy.deepcopy(dictOfDataFrames)\n",
    "for variable in dictOfDataFramesSubset.keys():\n",
    "    # Create a mask for rows matching the target coordinate\n",
    "    mask = (dictOfDataFramesSubset[variable]['lat'] == target_lat) & (dictOfDataFramesSubset[variable]['lon'] == target_lon)\n",
    "    dictOfDataFramesSubset[variable] = dictOfDataFramesSubset[variable][mask]\n",
    "\n",
    "# Now, we put all dataframes together in a single dataframe\n",
    "mergedDataframe = pd.merge(dictOfDataFramesSubset[\"rsds\"], dictOfDataFramesSubset[\"prc\"], on = [\"lat\", \"lon\", \"year\", \"month\", \"day\"], how='left')\n",
    "mergedDataframe = pd.merge(mergedDataframe, dictOfDataFramesSubset[\"tasmax\"], on = [\"lat\", \"lon\", \"year\", \"month\", \"day\"], how='left')\n",
    "mergedDataframe = pd.merge(mergedDataframe, dictOfDataFramesSubset[\"tasmin\"], on = [\"lat\", \"lon\", \"year\", \"month\", \"day\"], how='left')\n",
    "\n",
    "# WARNING : prc values are often 0. It's because it's daily precipitation data, it's normal !\n",
    "# print(mergedDataframe)\n",
    "\n",
    "##################################\n",
    "# Transforming units (important !)\n",
    "# And also variable names, to fit what is needed for the\n",
    "# LANDIS-II v8 climate library\n",
    "##################################\n",
    "\n",
    "# tasmin and tasmax are in kelvin. We put then in celcius.\n",
    "mergedDataframe['Tmin'] = mergedDataframe['tasmin'].apply(kelvin_to_celsius)\n",
    "mergedDataframe['Tmax'] = mergedDataframe['tasmax'].apply(kelvin_to_celsius)\n",
    "\n",
    "# Precipitations are in kg m-2 s-1 (see ESGF metagrid metadata for validation : https://esgf.nci.org.au/search?project=CMIP6&activeFacets=%7B%22nominal_resolution%22%3A%22100+km%22%2C%22frequency%22%3A%22day%22%2C%22experiment_id%22%3A%22historical%22%2C%22source_id%22%3A%22TaiESM1%22%2C%22variable_id%22%3A%22prc%22%7D)\n",
    "# Transforming precipitations from their original unit - kg m-2 s-1 - into cm per day\n",
    "mergedDataframe['precip'] = mergedDataframe['prc']*8640 # Going from seconds to days (86400 seconds in a day) and from mm (kg m-2) into cm (/10)\n",
    "\n",
    "# For solar radiation : we first need to transform it from daily average to daytime average.\n",
    "# Daily is on 24h (includes night); daily is average from sunrise to sunset.\n",
    "# The data pulled from ESGF is daily; it has been validated to me in an email exchange by \n",
    "# Gary Strand, data maintener of the dataset i used at first : http://doi.org/10.22033/ESGF/CMIP6.10071\n",
    "# We do that through a function that will calculate daytime hours based on the day of the year and the latitude/longitude.\n",
    "mergedDataframe[\"rsds_daytimeAverage\"] = [daylight_hour_average(row[\"rsds\"], row[\"lat\"], row[\"lon\"], row[\"year\"], row[\"month\"], row[\"day\"]) for index, row in mergedDataframe.iterrows()]\n",
    "\n",
    "# Transforming downwelling shortwave radiation from W/m2 to umol.m2/s-1\n",
    "# Based on the PnET User Guide's instructions\n",
    "# Downwelling shortwave radiation (rsds which we have here) is often refered as global solar radiation.\n",
    "# See https://library.wmo.int/viewer/68695/?offset=3#page=298&viewer=picture&o=search&n=0&q=shortwave . But other references exist.\n",
    "# So, Downwelling shortave radiation is for wavelengths of 0.2–4.0 μm; PAR is for 0.4–0.7 μm.\n",
    "# As such, to convert our Downwelling Shortwave Radiation in W/m2 to PAR in umol.m2/s-1, \n",
    "# we must multiply it by 2.02 as indicated in the user guide of PnET Succession.\n",
    "mergedDataframe['PAR'] = mergedDataframe['rsds']*2.02 # Going from seconds to days (86400 seconds in a day)\n",
    "\n",
    "# We remove the old columns\n",
    "mergedDataframe.drop('tasmin', axis=1, inplace=True)\n",
    "mergedDataframe.drop('tasmax', axis=1, inplace=True)\n",
    "mergedDataframe.drop('prc', axis=1, inplace=True)\n",
    "mergedDataframe.drop('rsds', axis=1, inplace=True)\n",
    "mergedDataframe.drop('rsds_daytimeAverage', axis=1, inplace=True)\n",
    "\n",
    "# We print the result\n",
    "# print(mergedDataframe)\n",
    "\n",
    "##################################\n",
    "# Giving the dataframe the right columns\n",
    "# for the LANDIS-II v8 climate library\n",
    "##################################\n",
    "\n",
    "# Create a unique identifier for each lat/lon pair\n",
    "mergedDataframe['lat_lon'] = mergedDataframe['lat'].astype(str) + '_' + mergedDataframe['lon'].astype(str)\n",
    "\n",
    "# Melt the dataframe to convert variables into rows\n",
    "melted_df = pd.melt(\n",
    "    mergedDataframe, \n",
    "    id_vars=['year', 'month', 'day', 'lat_lon'],\n",
    "    value_vars=['Tmin', 'Tmax', \"PAR\", \"precip\"],\n",
    "    var_name='Variable',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "# Pivot the table to get lat_lon as columns (which can then be used as ecoregions)\n",
    "result_df = melted_df.pivot_table(\n",
    "    index=['year', 'month', 'day', 'Variable'],\n",
    "    columns='lat_lon',\n",
    "    values='value'\n",
    ").reset_index()\n",
    "\n",
    "# Convert the column index from MultiIndex to regular Index\n",
    "result_df.columns.name = None\n",
    "result_df = result_df.sort_values(by=['Variable', 'year', 'month', 'day'])\n",
    "\n",
    "# We replace the name of the column that has the lat_lon pairing into an \n",
    "# ecoregion name (the one used in our PnET one cell scenario, eco1)\n",
    "result_df = result_df.rename(columns={result_df.columns[-1]: 'eco1'})\n",
    "# print(result_df)\n",
    "\n",
    "# We print as .csv\n",
    "result_df.to_csv(\"./ReferencesAndData/Climate Data/dataFrameClimate_historicalDaily.csv\", sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3c5ad0-d81c-4ab6-bb57-3c2f60575c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We finish by averaging the values (which is recommanded by the PnET User to use in the first steps of the calibration,\n",
    "# to have a very mild and stable climate with no extremes.\n",
    "# As a result, we will still have a climate stream with daily data from 1950 to 2010,\n",
    "# but every january 1st data for a given variable will the same, and will be the average of all january 1st in the original\n",
    "# non-averaged data, etc.\n",
    "from functionsForCalibration import *\n",
    "\n",
    "dataFrameClimate_historicalDaily = pd.read_csv(\"./ReferencesAndData/Climate Data/dataFrameClimate_historicalDaily.csv\")\n",
    "\n",
    "dataFrameClimate_historicalAveraged = transform_to_historical_averages_vectorized(dataFrameClimate_historicalDaily)\n",
    "\n",
    "# print(dataFrameClimate_historicalDaily)\n",
    "# print(dataFrameClimate_historicalAveraged)\n",
    "\n",
    "dataFrameClimate_historicalAveraged.to_csv(\"./ReferencesAndData/Climate Data/dataFrameClimate_historicalAveraged.csv\", sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae26d3a-e023-4e4b-b391-579f8cf9a5a2",
   "metadata": {},
   "source": [
    "### Climate source for ☁️ CO2 concentrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8487f9-6948-4c4c-a627-fb9b02afb64e",
   "metadata": {},
   "source": [
    "PnET-Succession needs CO2 concentrations in the air to simulation vegetation dynamic. However, it doesn't seem to be a variable that is generated by the GCMs of the CMIP6. It looks like it's because CO2 concentration are often assumed homogeneous on earth since it's a well-mixed gas, because it's an intrant/restriction to the models rather than an output, etc.\n",
    "\n",
    "Still, there are local variations around the atmosphere in CO2 concentrations (see [here](https://earthobservatory.nasa.gov/images/82142/global-patterns-of-carbon-dioxide)). The differences are small, and it seems like global values can be used (see [here](https://earthobservatory.nasa.gov/blogs/earthmatters/2016/12/05/reader-question-does-co2-disperse-evenly-around-the-earth/)). But I'm choosing here to try to get more nuanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb1f70-34db-4bbb-a6b4-75dbe5a1fd70",
   "metadata": {},
   "source": [
    "Data from https://www.nature.com/articles/s41597-022-01196-7#Sec5 seems very promising : available at https://zenodo.org/records/5021361, not too big, allows for local variations in CO2 concentration. Would need to script the download and treatment of the files here. Timestep is monthly, but we can downscale it to daily and still at least have some monthly variation. Goes to 2150, so perfect for our uses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d652ab-b4c5-4482-b473-d9eb03a5dbd2",
   "metadata": {},
   "source": [
    "Justification for using the dataset :\n",
    "\n",
    "- We know that this CO2 dataset does not come from the same GCM that generated our climate data, and there could be variability between the variability of climate variables in one dataset versus the climate variables in the models that generated these CO2 estimates at each given timestep.\n",
    "- However, We thought it better to use this dataset to approximate CO2 spatial and temporal variations in our simulations rather than using latitudinal or global averages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23f9c266-522a-4ebc-882a-1b03dcd8c56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Title: Global monthly distributions of atmospheric CO2 concentrations under the historical and future scenarios\n",
      "Keywords: Atmospheric chemistry, Atmospheric dynamics, Climate and Earth system modelling\n",
      "Publication date: 2021-06-23\n",
      "DOI: 10.5281/zenodo.5021361\n",
      "Total size: 1.0 GB\n",
      "\n",
      "Link: https://zenodo.org/records/5021361/files/CO2_1deg_month_1850-2013.nc   size: 1.0 GB\n",
      "\n",
      "Checksum is correct. (b11db6cc374ce23fd41b3b078cc8ebdd)\n",
      "All files have been downloaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we download the CO2 data from https://zenodo.org/records/5021361 \n",
    "\n",
    "from functionsForCalibration import *\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# We use the zenodo_get library to download files from Zenodo very fast; other methods tend to be slow.\n",
    "# See https://github.com/dvolgyes/zenodo_get . It is installed in the Docker image.\n",
    "# WARNING : will take around 1.5GB of space for the data from 1950 to 2150 for a full climate scenario.\n",
    "\n",
    "# We download the file with historical data\n",
    "os.system(\"cd ReferencesAndData && zenodo_get 5021361 -g CO2_1deg_month_1850-2013.nc\")\n",
    "\n",
    "# We can also download data for future conditions, like this :\n",
    "# os.system(\"cd ReferencesAndData && zenodo_get 5021361 -g CO2_SSP126_2015_2150.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59171d9d-9778-4cbd-9f14-c26cd32f06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No points found inside the polygon for historical data. Finding closest point...\n",
      "Closest point to polygon centroid for historical data: Lon=-83.5, Lat=47.5\n",
      "   year  month  day Variable  CO2_Concentration\n",
      "0  1950      1    1      CO2         317.369324\n",
      "1  1950      1    2      CO2         317.369324\n",
      "2  1950      1    3      CO2         317.369324\n",
      "3  1950      1    4      CO2         317.369324\n",
      "4  1950      1    5      CO2         317.369324\n",
      "       year  month  day Variable  CO2_Concentration\n",
      "22260  2010     12   27      CO2         399.926788\n",
      "22261  2010     12   28      CO2         399.926788\n",
      "22262  2010     12   29      CO2         399.926788\n",
      "22263  2010     12   30      CO2         399.926788\n",
      "22264  2010     12   31      CO2         399.926788\n"
     ]
    }
   ],
   "source": [
    "from functionsForCalibration import *\n",
    "\n",
    "historical_ds = xr.open_dataset(\"./ReferencesAndData/CO2_1deg_month_1850-2013.nc\")\n",
    "# future_ds = xr.open_dataset(\"./ReferencesAndData/CO2_SSP126_2015_2150.nc\")\n",
    "\n",
    "# Shapefile to select where we take our values; if several climate cells are in the shapefile,\n",
    "# then the CO2 values in all cells will be averaged (see function process_co2_data)\n",
    "shapefile = \"./ReferencesAndData/SpatialBoundaries/ChapleauBoundariesClimate.shp\"\n",
    "\n",
    "# We load the dataframe\n",
    "# We prepare the dataframe in a format that matches the older function I made\n",
    "# When I was using monthly data and LANDIS-II v7.\n",
    "# We load the file created previously\n",
    "df_climate = pd.read_csv(\"./ReferencesAndData/Climate Data/dataFrameClimate_historicalDaily.csv\")\n",
    "# Convert to year month day to string\n",
    "df_climate['date_string'] = (df_climate['year'].astype(str) + '-' + \n",
    "                     df_climate['month'].astype(str) + '-' + \n",
    "                     df_climate['day'].astype(str))\n",
    "# Get unique dates\n",
    "uniqueDates = df_climate['date_string'].unique()\n",
    "# Create the new dataframe\n",
    "dfToFill = pd.DataFrame(columns=[\"year\", \"month\", \"day\", \"Variable\"])\n",
    "# Put the dates in\n",
    "dfToFill[\"date_string\"] = uniqueDates\n",
    "# extract the dates correctly\n",
    "dfToFill['year'] = dfToFill[\"date_string\"].str.split('-').str[0].astype(int)\n",
    "dfToFill['month'] = dfToFill[\"date_string\"].str.split('-').str[1].astype(int)\n",
    "dfToFill['day'] = dfToFill[\"date_string\"].str.split('-').str[2].astype(int)\n",
    "# Add variable name\n",
    "dfToFill[\"Variable\"] = [\"CO2\"] * len(dfToFill)\n",
    "# Remove the date_string column\n",
    "dfToFill.drop('date_string', axis=1, inplace=True)\n",
    "df_climate.drop('date_string', axis=1, inplace=True)\n",
    "# Now, the dataframe is ready to be filled with CO2 values with the old function !\n",
    "# print(dfToFill)\n",
    "\n",
    "dfToFill = process_co2_data(historical_ds, shapefile, dfToFill)\n",
    "\n",
    "# Display the updated dataframe\n",
    "print(dfToFill.head())\n",
    "print(dfToFill.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c914dcb9-5634-437b-9268-814f49918d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        year  month  day Variable        eco1\n",
      "0       1950      1    1      PAR   37.535774\n",
      "1       1950      1    2      PAR  108.757004\n",
      "2       1950      1    3      PAR   72.965851\n",
      "3       1950      1    4      PAR  110.865730\n",
      "4       1950      1    5      PAR   92.960808\n",
      "...      ...    ...  ...      ...         ...\n",
      "155850  2010     12   27      CO2  399.926788\n",
      "155851  2010     12   28      CO2  399.926788\n",
      "155852  2010     12   29      CO2  399.926788\n",
      "155853  2010     12   30      CO2  399.926788\n",
      "155854  2010     12   31      CO2  399.926788\n",
      "\n",
      "[155855 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Finally, we add this CO2 data to the previous data, and re-average it if needed\n",
    "\n",
    "# First, we have to rename the CO2 column in the dataframe we just filled so that it corresponds to an ecoregion now\n",
    "# (this is due to the difference in the climate library formats with LANDIS-II v8 and the old functions I was using before switching)\n",
    "# The column name CO2_Concentration is created by the function process_co2_data\n",
    "dfToFill = dfToFill.rename(columns={'CO2_Concentration': 'eco1'})\n",
    "\n",
    "# We concatenate the dataframes\n",
    "df_climate = pd.concat([df_climate, dfToFill], ignore_index=True)\n",
    "\n",
    "# We check if needed\n",
    "print(df_climate)\n",
    "\n",
    "# We average\n",
    "dataFrameClimate_historicalAveraged = transform_to_historical_averages_vectorized(df_climate)\n",
    "# We save everything\n",
    "dataFrameClimate_historicalAveraged.to_csv(\"./ReferencesAndData/Climate Data/dataFrameClimate_historicalAveraged.csv\", sep=',', index=False, encoding='utf-8')\n",
    "\n",
    "# Now that everything is done, we can remove the CO2 files downloaded\n",
    "os.remove(\"./ReferencesAndData/CO2_1deg_month_1850-2013.nc\")\n",
    "# os.remove(\"./ReferencesAndData/CO2_SSP126_2015_2150.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81f6ef-7d1f-412b-b43c-ae0799a43b98",
   "metadata": {},
   "source": [
    "## Soils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989c33e-afb9-4be8-877b-5208f34989e1",
   "metadata": {},
   "source": [
    "Gustafon recommands SILO (Silty Loam/Silt Loam) soil from the default PnET Succession SaxtonAndRawls file (`C:\\Program Files\\LANDIS-II-v7\\extensions\\Defaults\\SaxtonAndRawlsParameters.txt`) as a soil that retains water well. This is because the first calibration step should use an \"ideal soil\" where water is well-retained so that lack of water (or watterlogging) does not influence growth at this calibration step.\n",
    "\n",
    "We can use the SILO soil type easily by modifying the SoilType of `eco1` in the PnET One Cell Scenario that we will use (see `SimulationFiles/PnETGitHub_OneCellSim_v8/`). So, nothing more to do here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f3375-f4d5-4f1b-b2db-619b5f2b7a62",
   "metadata": {},
   "source": [
    "## CLUES FOR FUTURE DATA\n",
    "\n",
    "- Alex Chubaty might have scripts to generate initial data in BC and Alberta, and even other provinces ?\n",
    "- Caren Dymonc for initial data in BC ?\n",
    "- Yan Boulanger : BioSIM climate data for small scale/downscaled climate data for PnET + BFOLDS ? Needed so that we can have PnET Climate input that change according to slope and topography in particular. Will need to create many smaller ecoregions that are combo of soils + climate \"zones\" in the area + classes of slopes/aspect. BioSIM might help in generating those things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ae3c2-bcc2-4a02-8c04-2195b02edbf6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
